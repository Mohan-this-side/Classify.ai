{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Importing backend modules...\n",
      "‚úÖ LLM Service reinitialized - Gemini client available: True\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, Image, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Fix environment variable\n",
    "os.environ['MAX_FILE_SIZE'] = '104857600'\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Set API keys BEFORE importing backend modules\n",
    "# This ensures the settings object reads them correctly\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyBGtJZtRFMIdoPgkEUG8UQPA6pBxUvmwSg' \n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['ANTHROPIC_API_KEY'] = ''\n",
    "\n",
    "# Or set a valid Gemini API key if you have one:\n",
    "# os.environ['GOOGLE_API_KEY'] = 'your-valid-api-key-here'\n",
    "\n",
    "# Add backend to path\n",
    "backend_path = Path(\"./backend\").resolve()\n",
    "if str(backend_path) not in sys.path:\n",
    "    sys.path.insert(0, str(backend_path))\n",
    "\n",
    "print(\"üì¶ Importing backend modules...\")\n",
    "\n",
    "# Import backend modules\n",
    "from app.workflows.state_management import state_manager\n",
    "from app.agents.data_cleaning.enhanced_data_cleaning_agent import EnhancedDataCleaningAgent\n",
    "from app.agents.data_analysis.eda_agent import EDAAgent\n",
    "from app.agents.ml_pipeline.feature_engineering_agent import FeatureEngineeringAgent\n",
    "from app.agents.ml_pipeline.ml_builder_agent import MLBuilderAgent\n",
    "from app.agents.ml_pipeline.model_evaluation_agent import ModelEvaluationAgent\n",
    "\n",
    "# Reinitialize LLM service to pick up API keys set after import\n",
    "# (This ensures the LLM service checks environment variables directly)\n",
    "from app.services.llm_service import get_llm_service, LLMProvider\n",
    "llm_service = get_llm_service(force_reinit=True)\n",
    "print(f\"‚úÖ LLM Service reinitialized - Gemini client available: {LLMProvider.GEMINI in llm_service.clients}\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# üéØ Multi-Agent ML Pipeline Visualization"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Visualizing outputs from each agent in the classification workflow*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded: 8,000 rows √ó 12 columns\n",
      "üéØ Target column: is_churned\n",
      "üìä Columns: ['user_id', 'gender', 'age', 'country', 'subscription_type', 'listening_time', 'songs_played_per_day', 'skip_rate', 'device_type', 'ads_listened_per_week', 'offline_listening', 'is_churned']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "display(Markdown(\"# üéØ Multi-Agent ML Pipeline Visualization\"))\n",
    "display(Markdown(\"*Visualizing outputs from each agent in the classification workflow*\\n\"))\n",
    "\n",
    "# Load your dataset\n",
    "DATASET_PATH = \"/Users/mgmanjusha/Classify.ai/test_data/spotify_churn_dataset.csv\"  \n",
    "TARGET_COLUMN = \"is_churned\"  # Change this\n",
    "WORKFLOW_ID = \"spotify_churn_demo\"  # Change this\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"üéØ Target column: {TARGET_COLUMN}\")\n",
    "print(f\"üìä Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üîÑ Initializing Workflow State"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ State initialized\n",
      "üì¶ State contains 82 fields for tracking agent outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE WORKFLOW STATE\n",
    "# ============================================================================\n",
    "\n",
    "display(Markdown(\"## üîÑ Initializing Workflow State\"))\n",
    "\n",
    "state = state_manager.initialize_state(\n",
    "    session_id=WORKFLOW_ID,\n",
    "    dataset_id=f\"dataset_{WORKFLOW_ID}\",\n",
    "    target_column=TARGET_COLUMN,\n",
    "    user_description=f\"Classification task for {TARGET_COLUMN}\",\n",
    "    api_key=\"demo\",\n",
    "    original_dataset=df\n",
    ")\n",
    "\n",
    "print(\"‚úÖ State initialized\")\n",
    "print(f\"üì¶ State contains {len(state)} fields for tracking agent outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# üßπ Data Cleaning Agent"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 16:50:07,624 - enhanced_data_cleaning - INFO - CodeValidator initialized successfully\n",
      "2025-10-29 16:50:07,624 - enhanced_data_cleaning - INFO - CodeValidator initialized successfully\n",
      "2025-10-29 16:50:07,624 - enhanced_data_cleaning - INFO - CodeValidator initialized successfully\n",
      "2025-10-29 16:50:07,624 - enhanced_data_cleaning - INFO - CodeValidator initialized successfully\n",
      "2025-10-29 16:50:07,624 - enhanced_data_cleaning - INFO - CodeValidator initialized successfully\n",
      "2025-10-29 16:50:07,624 - enhanced_data_cleaning - INFO - CodeValidator initialized successfully\n",
      "2025-10-29 16:50:07,637 - enhanced_data_cleaning - INFO - SandboxExecutor initialized (timeout=60s, memory=2g)\n",
      "2025-10-29 16:50:07,637 - enhanced_data_cleaning - INFO - SandboxExecutor initialized (timeout=60s, memory=2g)\n",
      "2025-10-29 16:50:07,637 - enhanced_data_cleaning - INFO - SandboxExecutor initialized (timeout=60s, memory=2g)\n",
      "2025-10-29 16:50:07,637 - enhanced_data_cleaning - INFO - SandboxExecutor initialized (timeout=60s, memory=2g)\n",
      "2025-10-29 16:50:07,637 - enhanced_data_cleaning - INFO - SandboxExecutor initialized (timeout=60s, memory=2g)\n",
      "2025-10-29 16:50:07,637 - enhanced_data_cleaning - INFO - SandboxExecutor initialized (timeout=60s, memory=2g)\n",
      "2025-10-29 16:50:07,660 - enhanced_data_cleaning - INFO - LLMService initialized successfully\n",
      "2025-10-29 16:50:07,660 - enhanced_data_cleaning - INFO - LLMService initialized successfully\n",
      "2025-10-29 16:50:07,660 - enhanced_data_cleaning - INFO - LLMService initialized successfully\n",
      "2025-10-29 16:50:07,660 - enhanced_data_cleaning - INFO - LLMService initialized successfully\n",
      "2025-10-29 16:50:07,660 - enhanced_data_cleaning - INFO - LLMService initialized successfully\n",
      "2025-10-29 16:50:07,660 - enhanced_data_cleaning - INFO - LLMService initialized successfully\n",
      "2025-10-29 16:50:07,694 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 2 ENABLED - All services initialized successfully\n",
      "2025-10-29 16:50:07,694 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 2 ENABLED - All services initialized successfully\n",
      "2025-10-29 16:50:07,694 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 2 ENABLED - All services initialized successfully\n",
      "2025-10-29 16:50:07,694 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 2 ENABLED - All services initialized successfully\n",
      "2025-10-29 16:50:07,694 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 2 ENABLED - All services initialized successfully\n",
      "2025-10-29 16:50:07,694 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 2 ENABLED - All services initialized successfully\n",
      "2025-10-29 16:50:07,729 - enhanced_data_cleaning - INFO - Starting double-layer execution for enhanced_data_cleaning\n",
      "2025-10-29 16:50:07,729 - enhanced_data_cleaning - INFO - Starting double-layer execution for enhanced_data_cleaning\n",
      "2025-10-29 16:50:07,729 - enhanced_data_cleaning - INFO - Starting double-layer execution for enhanced_data_cleaning\n",
      "2025-10-29 16:50:07,729 - enhanced_data_cleaning - INFO - Starting double-layer execution for enhanced_data_cleaning\n",
      "2025-10-29 16:50:07,729 - enhanced_data_cleaning - INFO - Starting double-layer execution for enhanced_data_cleaning\n",
      "2025-10-29 16:50:07,729 - enhanced_data_cleaning - INFO - Starting double-layer execution for enhanced_data_cleaning\n",
      "2025-10-29 16:50:07,738 - enhanced_data_cleaning - INFO - Executing Layer 1 (hardcoded analysis)...\n",
      "2025-10-29 16:50:07,738 - enhanced_data_cleaning - INFO - Executing Layer 1 (hardcoded analysis)...\n",
      "2025-10-29 16:50:07,738 - enhanced_data_cleaning - INFO - Executing Layer 1 (hardcoded analysis)...\n",
      "2025-10-29 16:50:07,738 - enhanced_data_cleaning - INFO - Executing Layer 1 (hardcoded analysis)...\n",
      "2025-10-29 16:50:07,738 - enhanced_data_cleaning - INFO - Executing Layer 1 (hardcoded analysis)...\n",
      "2025-10-29 16:50:07,738 - enhanced_data_cleaning - INFO - Executing Layer 1 (hardcoded analysis)...\n",
      "2025-10-29 16:50:07,743 - enhanced_data_cleaning - INFO - üîç LAYER 1: Starting hardcoded data cleaning ANALYSIS\n",
      "2025-10-29 16:50:07,743 - enhanced_data_cleaning - INFO - üîç LAYER 1: Starting hardcoded data cleaning ANALYSIS\n",
      "2025-10-29 16:50:07,743 - enhanced_data_cleaning - INFO - üîç LAYER 1: Starting hardcoded data cleaning ANALYSIS\n",
      "2025-10-29 16:50:07,743 - enhanced_data_cleaning - INFO - üîç LAYER 1: Starting hardcoded data cleaning ANALYSIS\n",
      "2025-10-29 16:50:07,743 - enhanced_data_cleaning - INFO - üîç LAYER 1: Starting hardcoded data cleaning ANALYSIS\n",
      "2025-10-29 16:50:07,743 - enhanced_data_cleaning - INFO - üîç LAYER 1: Starting hardcoded data cleaning ANALYSIS\n",
      "2025-10-29 16:50:07,752 - enhanced_data_cleaning - INFO - Analyzing dataset shape: (8000, 12)\n",
      "2025-10-29 16:50:07,752 - enhanced_data_cleaning - INFO - Analyzing dataset shape: (8000, 12)\n",
      "2025-10-29 16:50:07,752 - enhanced_data_cleaning - INFO - Analyzing dataset shape: (8000, 12)\n",
      "2025-10-29 16:50:07,752 - enhanced_data_cleaning - INFO - Analyzing dataset shape: (8000, 12)\n",
      "2025-10-29 16:50:07,752 - enhanced_data_cleaning - INFO - Analyzing dataset shape: (8000, 12)\n",
      "2025-10-29 16:50:07,752 - enhanced_data_cleaning - INFO - Analyzing dataset shape: (8000, 12)\n",
      "2025-10-29 16:50:07,759 - enhanced_data_cleaning - INFO - Starting comprehensive missing value analysis\n",
      "2025-10-29 16:50:07,759 - enhanced_data_cleaning - INFO - Starting comprehensive missing value analysis\n",
      "2025-10-29 16:50:07,759 - enhanced_data_cleaning - INFO - Starting comprehensive missing value analysis\n",
      "2025-10-29 16:50:07,759 - enhanced_data_cleaning - INFO - Starting comprehensive missing value analysis\n",
      "2025-10-29 16:50:07,759 - enhanced_data_cleaning - INFO - Starting comprehensive missing value analysis\n",
      "2025-10-29 16:50:07,759 - enhanced_data_cleaning - INFO - Starting comprehensive missing value analysis\n",
      "Error creating visualizations: [Errno 2] No such file or directory: '/app/results/missing_value_heatmap.png'\n",
      "2025-10-29 16:50:08,068 - enhanced_data_cleaning - INFO - Missing value analysis completed:\n",
      "2025-10-29 16:50:08,068 - enhanced_data_cleaning - INFO - Missing value analysis completed:\n",
      "2025-10-29 16:50:08,068 - enhanced_data_cleaning - INFO - Missing value analysis completed:\n",
      "2025-10-29 16:50:08,068 - enhanced_data_cleaning - INFO - Missing value analysis completed:\n",
      "2025-10-29 16:50:08,068 - enhanced_data_cleaning - INFO - Missing value analysis completed:\n",
      "2025-10-29 16:50:08,068 - enhanced_data_cleaning - INFO - Missing value analysis completed:\n",
      "2025-10-29 16:50:08,072 - enhanced_data_cleaning - INFO -   - Total missing values: 0\n",
      "2025-10-29 16:50:08,072 - enhanced_data_cleaning - INFO -   - Total missing values: 0\n",
      "2025-10-29 16:50:08,072 - enhanced_data_cleaning - INFO -   - Total missing values: 0\n",
      "2025-10-29 16:50:08,072 - enhanced_data_cleaning - INFO -   - Total missing values: 0\n",
      "2025-10-29 16:50:08,072 - enhanced_data_cleaning - INFO -   - Total missing values: 0\n",
      "2025-10-29 16:50:08,072 - enhanced_data_cleaning - INFO -   - Total missing values: 0\n",
      "2025-10-29 16:50:08,075 - enhanced_data_cleaning - INFO -   - Overall missing percentage: 0.0%\n",
      "2025-10-29 16:50:08,075 - enhanced_data_cleaning - INFO -   - Overall missing percentage: 0.0%\n",
      "2025-10-29 16:50:08,075 - enhanced_data_cleaning - INFO -   - Overall missing percentage: 0.0%\n",
      "2025-10-29 16:50:08,075 - enhanced_data_cleaning - INFO -   - Overall missing percentage: 0.0%\n",
      "2025-10-29 16:50:08,075 - enhanced_data_cleaning - INFO -   - Overall missing percentage: 0.0%\n",
      "2025-10-29 16:50:08,075 - enhanced_data_cleaning - INFO -   - Overall missing percentage: 0.0%\n",
      "2025-10-29 16:50:08,078 - enhanced_data_cleaning - INFO -   - Columns with missing values: 0\n",
      "2025-10-29 16:50:08,078 - enhanced_data_cleaning - INFO -   - Columns with missing values: 0\n",
      "2025-10-29 16:50:08,078 - enhanced_data_cleaning - INFO -   - Columns with missing values: 0\n",
      "2025-10-29 16:50:08,078 - enhanced_data_cleaning - INFO -   - Columns with missing values: 0\n",
      "2025-10-29 16:50:08,078 - enhanced_data_cleaning - INFO -   - Columns with missing values: 0\n",
      "2025-10-29 16:50:08,078 - enhanced_data_cleaning - INFO -   - Columns with missing values: 0\n",
      "2025-10-29 16:50:08,083 - enhanced_data_cleaning - INFO -   - Complete rows percentage: 100.0%\n",
      "2025-10-29 16:50:08,083 - enhanced_data_cleaning - INFO -   - Complete rows percentage: 100.0%\n",
      "2025-10-29 16:50:08,083 - enhanced_data_cleaning - INFO -   - Complete rows percentage: 100.0%\n",
      "2025-10-29 16:50:08,083 - enhanced_data_cleaning - INFO -   - Complete rows percentage: 100.0%\n",
      "2025-10-29 16:50:08,083 - enhanced_data_cleaning - INFO -   - Complete rows percentage: 100.0%\n",
      "2025-10-29 16:50:08,083 - enhanced_data_cleaning - INFO -   - Complete rows percentage: 100.0%\n",
      "2025-10-29 16:50:08,086 - enhanced_data_cleaning - INFO -   - MCAR independence score: 1.091\n",
      "2025-10-29 16:50:08,086 - enhanced_data_cleaning - INFO -   - MCAR independence score: 1.091\n",
      "2025-10-29 16:50:08,086 - enhanced_data_cleaning - INFO -   - MCAR independence score: 1.091\n",
      "2025-10-29 16:50:08,086 - enhanced_data_cleaning - INFO -   - MCAR independence score: 1.091\n",
      "2025-10-29 16:50:08,086 - enhanced_data_cleaning - INFO -   - MCAR independence score: 1.091\n",
      "2025-10-29 16:50:08,086 - enhanced_data_cleaning - INFO -   - MCAR independence score: 1.091\n",
      "2025-10-29 16:50:08,090 - enhanced_data_cleaning - INFO -   - Likely MCAR: True\n",
      "2025-10-29 16:50:08,090 - enhanced_data_cleaning - INFO -   - Likely MCAR: True\n",
      "2025-10-29 16:50:08,090 - enhanced_data_cleaning - INFO -   - Likely MCAR: True\n",
      "2025-10-29 16:50:08,090 - enhanced_data_cleaning - INFO -   - Likely MCAR: True\n",
      "2025-10-29 16:50:08,090 - enhanced_data_cleaning - INFO -   - Likely MCAR: True\n",
      "2025-10-29 16:50:08,090 - enhanced_data_cleaning - INFO -   - Likely MCAR: True\n",
      "2025-10-29 16:50:08,093 - enhanced_data_cleaning - INFO -   - Generated 2 recommendations\n",
      "2025-10-29 16:50:08,093 - enhanced_data_cleaning - INFO -   - Generated 2 recommendations\n",
      "2025-10-29 16:50:08,093 - enhanced_data_cleaning - INFO -   - Generated 2 recommendations\n",
      "2025-10-29 16:50:08,093 - enhanced_data_cleaning - INFO -   - Generated 2 recommendations\n",
      "2025-10-29 16:50:08,093 - enhanced_data_cleaning - INFO -   - Generated 2 recommendations\n",
      "2025-10-29 16:50:08,093 - enhanced_data_cleaning - INFO -   - Generated 2 recommendations\n",
      "2025-10-29 16:50:08,097 - enhanced_data_cleaning - INFO - Starting comprehensive data type validation\n",
      "2025-10-29 16:50:08,097 - enhanced_data_cleaning - INFO - Starting comprehensive data type validation\n",
      "2025-10-29 16:50:08,097 - enhanced_data_cleaning - INFO - Starting comprehensive data type validation\n",
      "2025-10-29 16:50:08,097 - enhanced_data_cleaning - INFO - Starting comprehensive data type validation\n",
      "2025-10-29 16:50:08,097 - enhanced_data_cleaning - INFO - Starting comprehensive data type validation\n",
      "2025-10-29 16:50:08,097 - enhanced_data_cleaning - INFO - Starting comprehensive data type validation\n",
      "2025-10-29 16:50:08,648 - enhanced_data_cleaning - INFO - Data type validation completed:\n",
      "2025-10-29 16:50:08,648 - enhanced_data_cleaning - INFO - Data type validation completed:\n",
      "2025-10-29 16:50:08,648 - enhanced_data_cleaning - INFO - Data type validation completed:\n",
      "2025-10-29 16:50:08,648 - enhanced_data_cleaning - INFO - Data type validation completed:\n",
      "2025-10-29 16:50:08,648 - enhanced_data_cleaning - INFO - Data type validation completed:\n",
      "2025-10-29 16:50:08,648 - enhanced_data_cleaning - INFO - Data type validation completed:\n",
      "2025-10-29 16:50:08,651 - enhanced_data_cleaning - INFO -   - Columns analyzed: 12\n",
      "2025-10-29 16:50:08,651 - enhanced_data_cleaning - INFO -   - Columns analyzed: 12\n",
      "2025-10-29 16:50:08,651 - enhanced_data_cleaning - INFO -   - Columns analyzed: 12\n",
      "2025-10-29 16:50:08,651 - enhanced_data_cleaning - INFO -   - Columns analyzed: 12\n",
      "2025-10-29 16:50:08,651 - enhanced_data_cleaning - INFO -   - Columns analyzed: 12\n",
      "2025-10-29 16:50:08,651 - enhanced_data_cleaning - INFO -   - Columns analyzed: 12\n",
      "2025-10-29 16:50:08,656 - enhanced_data_cleaning - INFO -   - Type appropriateness score: 0.97\n",
      "2025-10-29 16:50:08,656 - enhanced_data_cleaning - INFO -   - Type appropriateness score: 0.97\n",
      "2025-10-29 16:50:08,656 - enhanced_data_cleaning - INFO -   - Type appropriateness score: 0.97\n",
      "2025-10-29 16:50:08,656 - enhanced_data_cleaning - INFO -   - Type appropriateness score: 0.97\n",
      "2025-10-29 16:50:08,656 - enhanced_data_cleaning - INFO -   - Type appropriateness score: 0.97\n",
      "2025-10-29 16:50:08,656 - enhanced_data_cleaning - INFO -   - Type appropriateness score: 0.97\n",
      "2025-10-29 16:50:08,659 - enhanced_data_cleaning - INFO -   - Columns with issues: 0\n",
      "2025-10-29 16:50:08,659 - enhanced_data_cleaning - INFO -   - Columns with issues: 0\n",
      "2025-10-29 16:50:08,659 - enhanced_data_cleaning - INFO -   - Columns with issues: 0\n",
      "2025-10-29 16:50:08,659 - enhanced_data_cleaning - INFO -   - Columns with issues: 0\n",
      "2025-10-29 16:50:08,659 - enhanced_data_cleaning - INFO -   - Columns with issues: 0\n",
      "2025-10-29 16:50:08,659 - enhanced_data_cleaning - INFO -   - Columns with issues: 0\n",
      "2025-10-29 16:50:08,662 - enhanced_data_cleaning - INFO -   - Conversion needed: 0\n",
      "2025-10-29 16:50:08,662 - enhanced_data_cleaning - INFO -   - Conversion needed: 0\n",
      "2025-10-29 16:50:08,662 - enhanced_data_cleaning - INFO -   - Conversion needed: 0\n",
      "2025-10-29 16:50:08,662 - enhanced_data_cleaning - INFO -   - Conversion needed: 0\n",
      "2025-10-29 16:50:08,662 - enhanced_data_cleaning - INFO -   - Conversion needed: 0\n",
      "2025-10-29 16:50:08,662 - enhanced_data_cleaning - INFO -   - Conversion needed: 0\n",
      "2025-10-29 16:50:08,665 - enhanced_data_cleaning - INFO -   - Type distribution: {'numeric': 8, 'categorical': 4}\n",
      "2025-10-29 16:50:08,665 - enhanced_data_cleaning - INFO -   - Type distribution: {'numeric': 8, 'categorical': 4}\n",
      "2025-10-29 16:50:08,665 - enhanced_data_cleaning - INFO -   - Type distribution: {'numeric': 8, 'categorical': 4}\n",
      "2025-10-29 16:50:08,665 - enhanced_data_cleaning - INFO -   - Type distribution: {'numeric': 8, 'categorical': 4}\n",
      "2025-10-29 16:50:08,665 - enhanced_data_cleaning - INFO -   - Type distribution: {'numeric': 8, 'categorical': 4}\n",
      "2025-10-29 16:50:08,665 - enhanced_data_cleaning - INFO -   - Type distribution: {'numeric': 8, 'categorical': 4}\n",
      "2025-10-29 16:50:08,668 - enhanced_data_cleaning - INFO -   - Immediate conversions: 8\n",
      "2025-10-29 16:50:08,668 - enhanced_data_cleaning - INFO -   - Immediate conversions: 8\n",
      "2025-10-29 16:50:08,668 - enhanced_data_cleaning - INFO -   - Immediate conversions: 8\n",
      "2025-10-29 16:50:08,668 - enhanced_data_cleaning - INFO -   - Immediate conversions: 8\n",
      "2025-10-29 16:50:08,668 - enhanced_data_cleaning - INFO -   - Immediate conversions: 8\n",
      "2025-10-29 16:50:08,668 - enhanced_data_cleaning - INFO -   - Immediate conversions: 8\n",
      "2025-10-29 16:50:08,671 - enhanced_data_cleaning - INFO -   - Investigation needed: 4\n",
      "2025-10-29 16:50:08,671 - enhanced_data_cleaning - INFO -   - Investigation needed: 4\n",
      "2025-10-29 16:50:08,671 - enhanced_data_cleaning - INFO -   - Investigation needed: 4\n",
      "2025-10-29 16:50:08,671 - enhanced_data_cleaning - INFO -   - Investigation needed: 4\n",
      "2025-10-29 16:50:08,671 - enhanced_data_cleaning - INFO -   - Investigation needed: 4\n",
      "2025-10-29 16:50:08,671 - enhanced_data_cleaning - INFO -   - Investigation needed: 4\n",
      "2025-10-29 16:50:08,675 - enhanced_data_cleaning - INFO - Starting comprehensive outlier detection\n",
      "2025-10-29 16:50:08,675 - enhanced_data_cleaning - INFO - Starting comprehensive outlier detection\n",
      "2025-10-29 16:50:08,675 - enhanced_data_cleaning - INFO - Starting comprehensive outlier detection\n",
      "2025-10-29 16:50:08,675 - enhanced_data_cleaning - INFO - Starting comprehensive outlier detection\n",
      "2025-10-29 16:50:08,675 - enhanced_data_cleaning - INFO - Starting comprehensive outlier detection\n",
      "2025-10-29 16:50:08,675 - enhanced_data_cleaning - INFO - Starting comprehensive outlier detection\n",
      "Error creating visualizations: [Errno 2] No such file or directory: '/app/results/outlier_boxplots.png'\n",
      "2025-10-29 16:50:11,161 - enhanced_data_cleaning - INFO - Outlier detection completed:\n",
      "2025-10-29 16:50:11,161 - enhanced_data_cleaning - INFO - Outlier detection completed:\n",
      "2025-10-29 16:50:11,161 - enhanced_data_cleaning - INFO - Outlier detection completed:\n",
      "2025-10-29 16:50:11,161 - enhanced_data_cleaning - INFO - Outlier detection completed:\n",
      "2025-10-29 16:50:11,161 - enhanced_data_cleaning - INFO - Outlier detection completed:\n",
      "2025-10-29 16:50:11,161 - enhanced_data_cleaning - INFO - Outlier detection completed:\n",
      "2025-10-29 16:50:11,164 - enhanced_data_cleaning - INFO -   - Columns analyzed: 7\n",
      "2025-10-29 16:50:11,164 - enhanced_data_cleaning - INFO -   - Columns analyzed: 7\n",
      "2025-10-29 16:50:11,164 - enhanced_data_cleaning - INFO -   - Columns analyzed: 7\n",
      "2025-10-29 16:50:11,164 - enhanced_data_cleaning - INFO -   - Columns analyzed: 7\n",
      "2025-10-29 16:50:11,164 - enhanced_data_cleaning - INFO -   - Columns analyzed: 7\n",
      "2025-10-29 16:50:11,164 - enhanced_data_cleaning - INFO -   - Columns analyzed: 7\n",
      "2025-10-29 16:50:11,168 - enhanced_data_cleaning - INFO -   - Columns with outliers: 7\n",
      "2025-10-29 16:50:11,168 - enhanced_data_cleaning - INFO -   - Columns with outliers: 7\n",
      "2025-10-29 16:50:11,168 - enhanced_data_cleaning - INFO -   - Columns with outliers: 7\n",
      "2025-10-29 16:50:11,168 - enhanced_data_cleaning - INFO -   - Columns with outliers: 7\n",
      "2025-10-29 16:50:11,168 - enhanced_data_cleaning - INFO -   - Columns with outliers: 7\n",
      "2025-10-29 16:50:11,168 - enhanced_data_cleaning - INFO -   - Columns with outliers: 7\n",
      "2025-10-29 16:50:11,171 - enhanced_data_cleaning - INFO -   - Total outliers detected: 14743\n",
      "2025-10-29 16:50:11,171 - enhanced_data_cleaning - INFO -   - Total outliers detected: 14743\n",
      "2025-10-29 16:50:11,171 - enhanced_data_cleaning - INFO -   - Total outliers detected: 14743\n",
      "2025-10-29 16:50:11,171 - enhanced_data_cleaning - INFO -   - Total outliers detected: 14743\n",
      "2025-10-29 16:50:11,171 - enhanced_data_cleaning - INFO -   - Total outliers detected: 14743\n",
      "2025-10-29 16:50:11,171 - enhanced_data_cleaning - INFO -   - Total outliers detected: 14743\n",
      "2025-10-29 16:50:11,174 - enhanced_data_cleaning - INFO -   - Outlier percentage: 210.61%\n",
      "2025-10-29 16:50:11,174 - enhanced_data_cleaning - INFO -   - Outlier percentage: 210.61%\n",
      "2025-10-29 16:50:11,174 - enhanced_data_cleaning - INFO -   - Outlier percentage: 210.61%\n",
      "2025-10-29 16:50:11,174 - enhanced_data_cleaning - INFO -   - Outlier percentage: 210.61%\n",
      "2025-10-29 16:50:11,174 - enhanced_data_cleaning - INFO -   - Outlier percentage: 210.61%\n",
      "2025-10-29 16:50:11,174 - enhanced_data_cleaning - INFO -   - Outlier percentage: 210.61%\n",
      "2025-10-29 16:50:11,178 - enhanced_data_cleaning - INFO -   - Best performing method: isolation_forest\n",
      "2025-10-29 16:50:11,178 - enhanced_data_cleaning - INFO -   - Best performing method: isolation_forest\n",
      "2025-10-29 16:50:11,178 - enhanced_data_cleaning - INFO -   - Best performing method: isolation_forest\n",
      "2025-10-29 16:50:11,178 - enhanced_data_cleaning - INFO -   - Best performing method: isolation_forest\n",
      "2025-10-29 16:50:11,178 - enhanced_data_cleaning - INFO -   - Best performing method: isolation_forest\n",
      "2025-10-29 16:50:11,178 - enhanced_data_cleaning - INFO -   - Best performing method: isolation_forest\n",
      "2025-10-29 16:50:11,181 - enhanced_data_cleaning - INFO -   - Methods used: ['iqr', 'z_score', 'modified_z_score', 'isolation_forest', 'lof', 'percentile']\n",
      "2025-10-29 16:50:11,181 - enhanced_data_cleaning - INFO -   - Methods used: ['iqr', 'z_score', 'modified_z_score', 'isolation_forest', 'lof', 'percentile']\n",
      "2025-10-29 16:50:11,181 - enhanced_data_cleaning - INFO -   - Methods used: ['iqr', 'z_score', 'modified_z_score', 'isolation_forest', 'lof', 'percentile']\n",
      "2025-10-29 16:50:11,181 - enhanced_data_cleaning - INFO -   - Methods used: ['iqr', 'z_score', 'modified_z_score', 'isolation_forest', 'lof', 'percentile']\n",
      "2025-10-29 16:50:11,181 - enhanced_data_cleaning - INFO -   - Methods used: ['iqr', 'z_score', 'modified_z_score', 'isolation_forest', 'lof', 'percentile']\n",
      "2025-10-29 16:50:11,181 - enhanced_data_cleaning - INFO -   - Methods used: ['iqr', 'z_score', 'modified_z_score', 'isolation_forest', 'lof', 'percentile']\n",
      "2025-10-29 16:50:11,198 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 1: Analysis complete\n",
      "2025-10-29 16:50:11,198 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 1: Analysis complete\n",
      "2025-10-29 16:50:11,198 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 1: Analysis complete\n",
      "2025-10-29 16:50:11,198 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 1: Analysis complete\n",
      "2025-10-29 16:50:11,198 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 1: Analysis complete\n",
      "2025-10-29 16:50:11,198 - enhanced_data_cleaning - INFO - ‚úÖ LAYER 1: Analysis complete\n",
      "2025-10-29 16:50:11,203 - enhanced_data_cleaning - INFO - PERFORMANCE: Layer 1 execution took 3.46s\n",
      "2025-10-29 16:50:11,203 - enhanced_data_cleaning - INFO - PERFORMANCE: Layer 1 execution took 3.46s\n",
      "2025-10-29 16:50:11,203 - enhanced_data_cleaning - INFO - PERFORMANCE: Layer 1 execution took 3.46s\n",
      "2025-10-29 16:50:11,203 - enhanced_data_cleaning - INFO - PERFORMANCE: Layer 1 execution took 3.46s\n",
      "2025-10-29 16:50:11,203 - enhanced_data_cleaning - INFO - PERFORMANCE: Layer 1 execution took 3.46s\n",
      "2025-10-29 16:50:11,203 - enhanced_data_cleaning - INFO - PERFORMANCE: Layer 1 execution took 3.46s\n",
      "2025-10-29 16:50:11,205 - enhanced_data_cleaning - INFO - üîç Layer 2 Check: enable_layer2=True, can_use=True\n",
      "2025-10-29 16:50:11,205 - enhanced_data_cleaning - INFO - üîç Layer 2 Check: enable_layer2=True, can_use=True\n",
      "2025-10-29 16:50:11,205 - enhanced_data_cleaning - INFO - üîç Layer 2 Check: enable_layer2=True, can_use=True\n",
      "2025-10-29 16:50:11,205 - enhanced_data_cleaning - INFO - üîç Layer 2 Check: enable_layer2=True, can_use=True\n",
      "2025-10-29 16:50:11,205 - enhanced_data_cleaning - INFO - üîç Layer 2 Check: enable_layer2=True, can_use=True\n",
      "2025-10-29 16:50:11,205 - enhanced_data_cleaning - INFO - üîç Layer 2 Check: enable_layer2=True, can_use=True\n",
      "2025-10-29 16:50:11,209 - enhanced_data_cleaning - INFO - üöÄ Attempting Layer 2 (LLM + sandbox)...\n",
      "2025-10-29 16:50:11,209 - enhanced_data_cleaning - INFO - üöÄ Attempting Layer 2 (LLM + sandbox)...\n",
      "2025-10-29 16:50:11,209 - enhanced_data_cleaning - INFO - üöÄ Attempting Layer 2 (LLM + sandbox)...\n",
      "2025-10-29 16:50:11,209 - enhanced_data_cleaning - INFO - üöÄ Attempting Layer 2 (LLM + sandbox)...\n",
      "2025-10-29 16:50:11,209 - enhanced_data_cleaning - INFO - üöÄ Attempting Layer 2 (LLM + sandbox)...\n",
      "2025-10-29 16:50:11,209 - enhanced_data_cleaning - INFO - üöÄ Attempting Layer 2 (LLM + sandbox)...\n",
      "2025-10-29 16:50:11,213 - enhanced_data_cleaning - INFO - üìù Step 1/5: Generating Layer 2 code prompt...\n",
      "2025-10-29 16:50:11,213 - enhanced_data_cleaning - INFO - üìù Step 1/5: Generating Layer 2 code prompt...\n",
      "2025-10-29 16:50:11,213 - enhanced_data_cleaning - INFO - üìù Step 1/5: Generating Layer 2 code prompt...\n",
      "2025-10-29 16:50:11,213 - enhanced_data_cleaning - INFO - üìù Step 1/5: Generating Layer 2 code prompt...\n",
      "2025-10-29 16:50:11,213 - enhanced_data_cleaning - INFO - üìù Step 1/5: Generating Layer 2 code prompt...\n",
      "2025-10-29 16:50:11,213 - enhanced_data_cleaning - INFO - üìù Step 1/5: Generating Layer 2 code prompt...\n",
      "2025-10-29 16:50:11,216 - enhanced_data_cleaning - INFO - üîß LAYER 2: Generating LLM code generation prompt\n",
      "2025-10-29 16:50:11,216 - enhanced_data_cleaning - INFO - üîß LAYER 2: Generating LLM code generation prompt\n",
      "2025-10-29 16:50:11,216 - enhanced_data_cleaning - INFO - üîß LAYER 2: Generating LLM code generation prompt\n",
      "2025-10-29 16:50:11,216 - enhanced_data_cleaning - INFO - üîß LAYER 2: Generating LLM code generation prompt\n",
      "2025-10-29 16:50:11,216 - enhanced_data_cleaning - INFO - üîß LAYER 2: Generating LLM code generation prompt\n",
      "2025-10-29 16:50:11,216 - enhanced_data_cleaning - INFO - üîß LAYER 2: Generating LLM code generation prompt\n",
      "2025-10-29 16:50:11,219 - enhanced_data_cleaning - INFO -   ‚úÖ Generated prompt: 1413 characters\n",
      "2025-10-29 16:50:11,219 - enhanced_data_cleaning - INFO -   ‚úÖ Generated prompt: 1413 characters\n",
      "2025-10-29 16:50:11,219 - enhanced_data_cleaning - INFO -   ‚úÖ Generated prompt: 1413 characters\n",
      "2025-10-29 16:50:11,219 - enhanced_data_cleaning - INFO -   ‚úÖ Generated prompt: 1413 characters\n",
      "2025-10-29 16:50:11,219 - enhanced_data_cleaning - INFO -   ‚úÖ Generated prompt: 1413 characters\n",
      "2025-10-29 16:50:11,219 - enhanced_data_cleaning - INFO -   ‚úÖ Generated prompt: 1413 characters\n",
      "2025-10-29 16:50:11,223 - enhanced_data_cleaning - INFO - ü§ñ Step 2/5: Calling LLM to generate code...\n",
      "2025-10-29 16:50:11,223 - enhanced_data_cleaning - INFO - ü§ñ Step 2/5: Calling LLM to generate code...\n",
      "2025-10-29 16:50:11,223 - enhanced_data_cleaning - INFO - ü§ñ Step 2/5: Calling LLM to generate code...\n",
      "2025-10-29 16:50:11,223 - enhanced_data_cleaning - INFO - ü§ñ Step 2/5: Calling LLM to generate code...\n",
      "2025-10-29 16:50:11,223 - enhanced_data_cleaning - INFO - ü§ñ Step 2/5: Calling LLM to generate code...\n",
      "2025-10-29 16:50:11,223 - enhanced_data_cleaning - INFO - ü§ñ Step 2/5: Calling LLM to generate code...\n",
      "2025-10-29 16:50:11,228 - enhanced_data_cleaning - INFO -   - LLM Service: LLMService\n",
      "2025-10-29 16:50:11,228 - enhanced_data_cleaning - INFO -   - LLM Service: LLMService\n",
      "2025-10-29 16:50:11,228 - enhanced_data_cleaning - INFO -   - LLM Service: LLMService\n",
      "2025-10-29 16:50:11,228 - enhanced_data_cleaning - INFO -   - LLM Service: LLMService\n",
      "2025-10-29 16:50:11,228 - enhanced_data_cleaning - INFO -   - LLM Service: LLMService\n",
      "2025-10-29 16:50:11,228 - enhanced_data_cleaning - INFO -   - LLM Service: LLMService\n",
      "Error with gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "Error with gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "Error with gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "2025-10-29 16:50:12,402 - enhanced_data_cleaning - ERROR - Layer 2 execution failed: Failed to generate code with all providers\n",
      "2025-10-29 16:50:12,402 - enhanced_data_cleaning - ERROR - Layer 2 execution failed: Failed to generate code with all providers\n",
      "2025-10-29 16:50:12,402 - enhanced_data_cleaning - ERROR - Layer 2 execution failed: Failed to generate code with all providers\n",
      "2025-10-29 16:50:12,402 - enhanced_data_cleaning - ERROR - Layer 2 execution failed: Failed to generate code with all providers\n",
      "2025-10-29 16:50:12,402 - enhanced_data_cleaning - ERROR - Layer 2 execution failed: Failed to generate code with all providers\n",
      "2025-10-29 16:50:12,402 - enhanced_data_cleaning - ERROR - Layer 2 execution failed: Failed to generate code with all providers\n",
      "2025-10-29 16:50:12,405 - enhanced_data_cleaning - WARNING - ‚ùå Layer 2 failed: Failed to generate code with all providers, falling back to Layer 1\n",
      "2025-10-29 16:50:12,405 - enhanced_data_cleaning - WARNING - ‚ùå Layer 2 failed: Failed to generate code with all providers, falling back to Layer 1\n",
      "2025-10-29 16:50:12,405 - enhanced_data_cleaning - WARNING - ‚ùå Layer 2 failed: Failed to generate code with all providers, falling back to Layer 1\n",
      "2025-10-29 16:50:12,405 - enhanced_data_cleaning - WARNING - ‚ùå Layer 2 failed: Failed to generate code with all providers, falling back to Layer 1\n",
      "2025-10-29 16:50:12,405 - enhanced_data_cleaning - WARNING - ‚ùå Layer 2 failed: Failed to generate code with all providers, falling back to Layer 1\n",
      "2025-10-29 16:50:12,405 - enhanced_data_cleaning - WARNING - ‚ùå Layer 2 failed: Failed to generate code with all providers, falling back to Layer 1\n",
      "2025-10-29 16:50:12,409 - enhanced_data_cleaning - INFO - ‚úÖ Copied dataset_shape to state from enhanced_data_cleaning (type: tuple)\n",
      "2025-10-29 16:50:12,409 - enhanced_data_cleaning - INFO - ‚úÖ Copied dataset_shape to state from enhanced_data_cleaning (type: tuple)\n",
      "2025-10-29 16:50:12,409 - enhanced_data_cleaning - INFO - ‚úÖ Copied dataset_shape to state from enhanced_data_cleaning (type: tuple)\n",
      "2025-10-29 16:50:12,409 - enhanced_data_cleaning - INFO - ‚úÖ Copied dataset_shape to state from enhanced_data_cleaning (type: tuple)\n",
      "2025-10-29 16:50:12,409 - enhanced_data_cleaning - INFO - ‚úÖ Copied dataset_shape to state from enhanced_data_cleaning (type: tuple)\n",
      "2025-10-29 16:50:12,409 - enhanced_data_cleaning - INFO - ‚úÖ Copied dataset_shape to state from enhanced_data_cleaning (type: tuple)\n",
      "2025-10-29 16:50:12,413 - enhanced_data_cleaning - INFO - ‚úÖ Copied columns to state from enhanced_data_cleaning (type: list)\n",
      "2025-10-29 16:50:12,413 - enhanced_data_cleaning - INFO - ‚úÖ Copied columns to state from enhanced_data_cleaning (type: list)\n",
      "2025-10-29 16:50:12,413 - enhanced_data_cleaning - INFO - ‚úÖ Copied columns to state from enhanced_data_cleaning (type: list)\n",
      "2025-10-29 16:50:12,413 - enhanced_data_cleaning - INFO - ‚úÖ Copied columns to state from enhanced_data_cleaning (type: list)\n",
      "2025-10-29 16:50:12,413 - enhanced_data_cleaning - INFO - ‚úÖ Copied columns to state from enhanced_data_cleaning (type: list)\n",
      "2025-10-29 16:50:12,413 - enhanced_data_cleaning - INFO - ‚úÖ Copied columns to state from enhanced_data_cleaning (type: list)\n",
      "2025-10-29 16:50:12,416 - enhanced_data_cleaning - INFO - ‚úÖ Copied data_types to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,416 - enhanced_data_cleaning - INFO - ‚úÖ Copied data_types to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,416 - enhanced_data_cleaning - INFO - ‚úÖ Copied data_types to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,416 - enhanced_data_cleaning - INFO - ‚úÖ Copied data_types to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,416 - enhanced_data_cleaning - INFO - ‚úÖ Copied data_types to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,416 - enhanced_data_cleaning - INFO - ‚úÖ Copied data_types to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,419 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_values to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,419 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_values to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,419 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_values to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,419 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_values to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,419 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_values to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,419 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_values to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,422 - enhanced_data_cleaning - INFO - ‚úÖ Copied duplicate_count to state from enhanced_data_cleaning (type: int)\n",
      "2025-10-29 16:50:12,422 - enhanced_data_cleaning - INFO - ‚úÖ Copied duplicate_count to state from enhanced_data_cleaning (type: int)\n",
      "2025-10-29 16:50:12,422 - enhanced_data_cleaning - INFO - ‚úÖ Copied duplicate_count to state from enhanced_data_cleaning (type: int)\n",
      "2025-10-29 16:50:12,422 - enhanced_data_cleaning - INFO - ‚úÖ Copied duplicate_count to state from enhanced_data_cleaning (type: int)\n",
      "2025-10-29 16:50:12,422 - enhanced_data_cleaning - INFO - ‚úÖ Copied duplicate_count to state from enhanced_data_cleaning (type: int)\n",
      "2025-10-29 16:50:12,422 - enhanced_data_cleaning - INFO - ‚úÖ Copied duplicate_count to state from enhanced_data_cleaning (type: int)\n",
      "2025-10-29 16:50:12,425 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_analysis to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,425 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_analysis to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,425 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_analysis to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,425 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_analysis to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,425 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_analysis to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,425 - enhanced_data_cleaning - INFO - ‚úÖ Copied missing_analysis to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,429 - enhanced_data_cleaning - INFO - ‚úÖ Copied type_validation to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,429 - enhanced_data_cleaning - INFO - ‚úÖ Copied type_validation to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,429 - enhanced_data_cleaning - INFO - ‚úÖ Copied type_validation to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,429 - enhanced_data_cleaning - INFO - ‚úÖ Copied type_validation to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,429 - enhanced_data_cleaning - INFO - ‚úÖ Copied type_validation to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,429 - enhanced_data_cleaning - INFO - ‚úÖ Copied type_validation to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,433 - enhanced_data_cleaning - INFO - ‚úÖ Copied outlier_detection to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,433 - enhanced_data_cleaning - INFO - ‚úÖ Copied outlier_detection to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,433 - enhanced_data_cleaning - INFO - ‚úÖ Copied outlier_detection to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,433 - enhanced_data_cleaning - INFO - ‚úÖ Copied outlier_detection to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,433 - enhanced_data_cleaning - INFO - ‚úÖ Copied outlier_detection to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,433 - enhanced_data_cleaning - INFO - ‚úÖ Copied outlier_detection to state from enhanced_data_cleaning (type: dict)\n",
      "2025-10-29 16:50:12,436 - enhanced_data_cleaning - INFO - ‚úÖ Copied target_column to state from enhanced_data_cleaning (type: str)\n",
      "2025-10-29 16:50:12,436 - enhanced_data_cleaning - INFO - ‚úÖ Copied target_column to state from enhanced_data_cleaning (type: str)\n",
      "2025-10-29 16:50:12,436 - enhanced_data_cleaning - INFO - ‚úÖ Copied target_column to state from enhanced_data_cleaning (type: str)\n",
      "2025-10-29 16:50:12,436 - enhanced_data_cleaning - INFO - ‚úÖ Copied target_column to state from enhanced_data_cleaning (type: str)\n",
      "2025-10-29 16:50:12,436 - enhanced_data_cleaning - INFO - ‚úÖ Copied target_column to state from enhanced_data_cleaning (type: str)\n",
      "2025-10-29 16:50:12,436 - enhanced_data_cleaning - INFO - ‚úÖ Copied target_column to state from enhanced_data_cleaning (type: str)\n",
      "2025-10-29 16:50:12,439 - enhanced_data_cleaning - INFO - üì¶ Total keys copied from enhanced_data_cleaning: 9\n",
      "2025-10-29 16:50:12,439 - enhanced_data_cleaning - INFO - üì¶ Total keys copied from enhanced_data_cleaning: 9\n",
      "2025-10-29 16:50:12,439 - enhanced_data_cleaning - INFO - üì¶ Total keys copied from enhanced_data_cleaning: 9\n",
      "2025-10-29 16:50:12,439 - enhanced_data_cleaning - INFO - üì¶ Total keys copied from enhanced_data_cleaning: 9\n",
      "2025-10-29 16:50:12,439 - enhanced_data_cleaning - INFO - üì¶ Total keys copied from enhanced_data_cleaning: 9\n",
      "2025-10-29 16:50:12,439 - enhanced_data_cleaning - INFO - üì¶ Total keys copied from enhanced_data_cleaning: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Status: pending\n",
      "\n",
      "üîç Issues Found (0):\n",
      "  ‚úÖ No issues detected!\n",
      "\n",
      "‚úÖ Actions Taken (0):\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA CLEANING AGENT\n",
    "# ============================================================================\n",
    "\n",
    "display(Markdown(\"# üßπ Data Cleaning Agent\"))\n",
    "\n",
    "cleaning_agent = EnhancedDataCleaningAgent()\n",
    "state = await cleaning_agent.execute(state)\n",
    "\n",
    "print(f\"‚úÖ Status: {state.get('agent_statuses', {}).get('data_cleaning')}\")\n",
    "\n",
    "# Quality Score\n",
    "quality_score = state.get('data_quality_score')\n",
    "if quality_score:\n",
    "    print(f\"\\nüìä Data Quality Score: {quality_score:.2%}\")\n",
    "    \n",
    "    # Visualize quality score\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "    color = 'green' if quality_score > 0.8 else 'orange' if quality_score > 0.6 else 'red'\n",
    "    ax.barh(['Quality Score'], [quality_score], color=color, alpha=0.7)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title('Data Quality Score')\n",
    "    ax.axvline(x=0.8, color='green', linestyle='--', alpha=0.3, label='Good (>0.8)')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Issues Found\n",
    "issues = state.get('cleaning_issues_found', [])\n",
    "print(f\"\\nüîç Issues Found ({len(issues)}):\")\n",
    "if issues:\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No issues detected!\")\n",
    "\n",
    "# Actions Taken\n",
    "actions = state.get('cleaning_actions_taken', [])\n",
    "print(f\"\\n‚úÖ Actions Taken ({len(actions)}):\")\n",
    "for i, action in enumerate(actions, 1):\n",
    "    print(f\"  {i}. {action}\")\n",
    "\n",
    "# Cleaning Summary\n",
    "cleaning_summary = state.get('cleaning_summary')\n",
    "if cleaning_summary:\n",
    "    print(f\"\\nüìã Cleaning Summary:\")\n",
    "    print(cleaning_summary[:500])  # First 500 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (4121664803.py, line 122)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 122\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"\\n  ‚ö†Ô∏è No stro\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EDA AGENT\n",
    "# ============================================================================\n",
    "\n",
    "display(Markdown(\"# üìä Exploratory Data Analysis Agent\"))\n",
    "\n",
    "# Store dataset for EDA\n",
    "state_manager.store_dataset(state, df, dataset_type=\"cleaned\")\n",
    "\n",
    "eda_agent = EDAAgent()\n",
    "state = await eda_agent.execute(state)\n",
    "\n",
    "print(f\"‚úÖ Status: {state.get('agent_statuses', {}).get('eda_analysis')}\")\n",
    "\n",
    "# Statistical Summary\n",
    "display(Markdown(\"\\n## üìà Dataset Statistics\"))\n",
    "\n",
    "stats = state.get('statistical_summary', {})\n",
    "if stats:\n",
    "    print(f\"Dataset Overview:\")\n",
    "    dataset_shape = stats.get('dataset_shape', {})\n",
    "    print(f\"  - Shape: {dataset_shape.get('rows', 0):,} rows √ó {dataset_shape.get('columns', 0)} columns\")\n",
    "    \n",
    "    data_types = stats.get('data_types', {})\n",
    "    print(f\"  - Numeric Features: {data_types.get('numeric', 0)}\")\n",
    "    print(f\"  - Categorical Features: {data_types.get('categorical', 0)}\")\n",
    "    \n",
    "    missing = stats.get('missing_values', {})\n",
    "    print(f\"  - Missing Values: {missing.get('total', 0)} ({missing.get('percentage', 0):.2f}%)\")\n",
    "    \n",
    "    duplicates = stats.get('duplicates', {})\n",
    "    print(f\"  - Duplicates: {duplicates.get('exact_duplicates', 0)} ({duplicates.get('duplicate_percentage', 0):.2f}%)\")\n",
    "\n",
    "# Target Analysis\n",
    "display(Markdown(\"\\n## üéØ Target Variable Analysis\"))\n",
    "\n",
    "target_analysis = state.get('target_analysis', {})\n",
    "if target_analysis:\n",
    "    distribution = target_analysis.get('distribution', {})\n",
    "    print(f\"Target Variable: {TARGET_COLUMN}\")\n",
    "    print(f\"  - Unique Values: {distribution.get('unique_values', 'N/A')}\")\n",
    "    print(f\"  - Missing: {distribution.get('missing_count', 0)} ({distribution.get('missing_percentage', 0):.2f}%)\")\n",
    "    \n",
    "    # Class balance\n",
    "    if 'class_balance' in target_analysis:\n",
    "        balance = target_analysis['class_balance']\n",
    "        balanced = \"‚úÖ Balanced\" if balance.get('is_balanced') else \"‚ö†Ô∏è Imbalanced\"\n",
    "        print(f\"  - Class Balance: {balanced}\")\n",
    "        print(f\"  - Balance Ratio: {balance.get('balance_ratio', 0):.2f}\")\n",
    "        print(f\"  - Majority Class: {balance.get('majority_class')} ({balance.get('majority_count', 0):,} samples)\")\n",
    "        print(f\"  - Minority Class: {balance.get('minority_class')} ({balance.get('minority_count', 0):,} samples)\")\n",
    "    \n",
    "    # Value counts visualization\n",
    "    value_counts = distribution.get('value_counts', {})\n",
    "    if value_counts:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        classes = list(value_counts.keys())\n",
    "        counts = list(value_counts.values())\n",
    "        ax.bar([str(c) for c in classes], counts, color=['skyblue', 'coral'])\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'Target Distribution: {TARGET_COLUMN}')\n",
    "        for i, (c, count) in enumerate(zip(classes, counts)):\n",
    "            ax.text(i, count, f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Target insights\n",
    "    insights = target_analysis.get('insights', [])\n",
    "    if insights:\n",
    "        print(f\"\\nüí° Key Insights:\")\n",
    "        for insight in insights:\n",
    "            print(f\"  ‚Ä¢ {insight}\")\n",
    "\n",
    "# Distribution Analysis\n",
    "display(Markdown(\"\\n## üìä Feature Distributions\"))\n",
    "\n",
    "dist_analysis = state.get('distribution_analysis', {})\n",
    "if dist_analysis:\n",
    "    dist_stats = dist_analysis.get('distribution_statistics', {})\n",
    "    if dist_stats:\n",
    "        print(f\"Distribution Statistics for {len(dist_stats)} numeric features:\")\n",
    "        \n",
    "        # Create summary table\n",
    "        dist_df = pd.DataFrame.from_dict(dist_stats, orient='index')\n",
    "        dist_df = dist_df.round(2)\n",
    "        display(dist_df.head(10))  # Show first 10\n",
    "    \n",
    "    insights = dist_analysis.get('insights', [])\n",
    "    if insights:\n",
    "        print(f\"\\nüí° Distribution Insights:\")\n",
    "        for insight in insights[:5]:  # Top 5\n",
    "            print(f\"  ‚Ä¢ {insight}\")\n",
    "\n",
    "# Correlation Analysis\n",
    "display(Markdown(\"\\n## üîó Correlation Analysis\"))\n",
    "\n",
    "corr_analysis = state.get('correlation_analysis', {})\n",
    "if corr_analysis:\n",
    "    target_corr = corr_analysis.get('target_correlations', {})\n",
    "    \n",
    "    if target_corr:\n",
    "        strong_pos = target_corr.get('strong_positive', {})\n",
    "        strong_neg = target_corr.get('strong_negative', {})\n",
    "        moderate_pos = target_corr.get('moderate_positive', {})\n",
    "        moderate_neg = target_corr.get('moderate_negative', {})\n",
    "        weak = target_corr.get('weak', {})\n",
    "        \n",
    "        print(f\"Correlations with {TARGET_COLUMN}:\")\n",
    "        \n",
    "        if strong_pos:\n",
    "            print(f\"\\n  üî¥ Strong Positive (>0.7): {len(strong_pos)} features\")\n",
    "            for feat, corr in list(strong_pos.items())[:3]:\n",
    "                print(f\"    - {feat}: {corr:.3f}\")\n",
    "        \n",
    "        if strong_neg:\n",
    "            print(f\"\\n  üîµ Strong Negative (<-0.7): {len(strong_neg)} features\")\n",
    "            for feat, corr in list(strong_neg.items())[:3]:\n",
    "                print(f\"    - {feat}: {corr:.3f}\")\n",
    "        \n",
    "        if not strong_pos and not strong_neg:\n",
    "            print(f\"\\n  ‚ö†Ô∏è No stro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
